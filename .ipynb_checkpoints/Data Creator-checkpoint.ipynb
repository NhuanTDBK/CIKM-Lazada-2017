{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import seaborn\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uri_re = r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))'\n",
    "\n",
    "def stripTagsAndUris(x):\n",
    "    if x:\n",
    "        # BeautifulSoup on content\n",
    "        soup = BeautifulSoup(x, \"html.parser\")\n",
    "        # Stripping all <code> tags with their content if any\n",
    "        if soup.code:\n",
    "            soup.code.decompose()\n",
    "        # Get all the text out of the html\n",
    "        text =  soup.get_text()\n",
    "        # Returning text stripping out all uris\n",
    "        return re.sub(uri_re, \"\", text)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def removePunctuation(x):\n",
    "    # Lowercasing all words\n",
    "    x = x.lower()\n",
    "    # Removing non ASCII chars\n",
    "    x = re.sub(r'[^\\x00-\\x7f]',r' ',x)\n",
    "    # Removing (replacing with empty spaces actually) all the punctuations\n",
    "    return re.sub(\"[\"+string.punctuation+\"]\", \" \", x)\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "def removeStopword(sentence):\n",
    "    return ' '.join(set([lemmatizer.lemmatize(w.lower()) for w in nltk.wordpunct_tokenize(sentence) \n",
    " if ((w not in stopwords.words('english')) and (w not in string.punctuation))]))\n",
    "def getSentenceLength(sen):\n",
    "    return len(sen.split())\n",
    "def normalize_df(filename):\n",
    "    columns = [\"country\",\"sku_id\",\"title\",\"category_lvl_1\",\"category_lvl_2\",\"category_lvl_3\",\"short_description\",\n",
    "           \"price\",\"product_type\"]\n",
    "    dat = pd.read_csv(filename,names=columns)\n",
    "    dat['short_desc_strip'] = dat['short_description'].fillna(\"\").map(stripTagsAndUris).map(removeStopword)\n",
    "    dat['title'] = dat['title'].fillna(\"\").map(stripTagsAndUris).map(removeStopword)\n",
    "    dat['len_title'] = dat['title'].map(getSentenceLength)\n",
    "    dat['len_desc'] = dat['short_desc_strip'].map(getSentenceLength)\n",
    "    return dat.drop(['sku_id','short_description'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiColumnLabelEncoder:\n",
    "    def __init__(self,columns = None):\n",
    "        self.columns = columns # array of column names to encode\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self # not relevant here\n",
    "\n",
    "    def transform(self,X):\n",
    "        '''\n",
    "        Transforms columns of X specified in self.columns using\n",
    "        LabelEncoder(). If no columns specified, transforms all\n",
    "        columns in X.\n",
    "        '''\n",
    "        output = X.copy()\n",
    "        if self.columns is not None:\n",
    "            for col in self.columns:\n",
    "                output[col] = LabelEncoder().fit_transform(output[col])\n",
    "        else:\n",
    "            for colname,col in output.iteritems():\n",
    "                output[colname] = LabelEncoder().fit_transform(col)\n",
    "        return output\n",
    "\n",
    "    def fit_transform(self,X,y=None):\n",
    "        return self.fit(X,y).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = [\"country\",\"sku_id\",\"title\",\"category_lvl_1\",\"category_lvl_2\",\"category_lvl_3\",\"short_description\",\n",
    "           \"price\",\"product_type\"]\n",
    "# dat = pd.read_csv(\"data/training/data_train.csv\",names=columns)\n",
    "df_train = normalize_df(\"data/training/data_train.csv\")\n",
    "df_val = normalize_df(\"data/validation/data_valid.csv\")\n",
    "clarity_lbl = pd.read_csv(\"data/training/clarity_train.labels\",names=[\"label\"])\n",
    "concise_lbl = pd.read_csv(\"data/training/conciseness_train.labels\",names=[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_total = df_train.append(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns_to_categorical = ['country','category_lvl_1','category_lvl_2','category_lvl_3','product_type']\n",
    "df_total_encoder = MultiColumnLabelEncoder(columns=columns_to_categorical).fit_transform(df_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_total_encoder.to_csv('data/total_df_with_categorical_and_normalize.csv',index=0,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_total_encoder[len(df_train):].to_csv(\"data/validation/data_valid_normalize.csv\",index=0,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_norm = df_total_encoder[:len(df_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train_norm['clarity'] = clarity_lbl\n",
    "df_train_norm['concise'] = concise_lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_norm.to_csv(\"data/training/data_train_normalize.csv\",index=0,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_test = train_test_split(df_train_norm,clarity_lbl,stratify=clarity_lbl,random_state=4111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.to_csv(\"train_cat.csv\",index=None,encoding='utf-8')\n",
    "X_val.to_csv(\"val_cat.csv\",index=None,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
